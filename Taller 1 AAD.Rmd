---
title: "Taller 1 Analisis Avanzado de Datos"
subtitle: "Maestria en Matemáticas Aplicadas y Ciencias de la Computación"
author: 
  - "Carlos Daniel Barriga"
  - "Fabian Ricardo Luengas"
date: "2024-03-03"
output:
  rmdformats::downcute
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Taller 1

*Problema:*
El conjunto de datos *taller1*.txt contiene la información del perfíl genómico de un conjunto de 1200 líneas celulares. Para estas se busca determinar cuáles de los 5000 genes (ubicados en cada columna) son de relevancia para la predicción de la variable respuesta (efectividad del tratamiento anticancer, medida como variable continua).

```{r}
datos <- read.csv('taller1.txt')
```

## 1- Hay multicolinealidad en los datos?

si, hay multicolinealidad en los datos, debido a que hay mas variables que registros.

## 2- Separacion de los datos

```{r}
set.seed(47)

inds <- sample(1200,1000)

train <- datos[inds,]
test <- datos[-inds,]

```

## 3- validacion cruzada para $\lambda\ $ en Ridge y Lasso

Se usa Validación cruzada en k - folds para 5 folds

```{r warning=FALSE, echo=FALSE}
library(glmnet)
rownames(train) <- NULL
X <- as.matrix(train[,2:5001])
Y <- train[,1]

lambda = seq(0.001,1,length=100)
```


### 3.1 Regresión Ridge

```{r}

valnet <- cv.glmnet(X,Y, alpha = 0, nfolds = 5, lambda = lambda)

lambda_ridge = valnet$lambda.min
```

```{r}

plot(valnet$lambda,valnet$cvm, type = 'l', xlab = "Lambda", ylab = "ECM")
abline(v = valnet$lambda.min, col = 'red')

```

Como se puede observar en la gráfica el mejor $\lambda_r\ $ para la regresión ridge es `r valnet$lambda.min`

### 3.2 regresión Lasso

```{r warning=FALSE}

valnetlasso <- cv.glmnet(X,Y, nfolds = 5, lambda = lambda)

ecmls <- valnetlasso$cvm
```

```{r}

plot(valnetlasso$lambda,valnetlasso$cvm, type = 'l',xlab = "Lambda", ylab = "ECM")
abline(v = valnetlasso$lambda.min, col = 'red')

lambda_lasso = valnetlasso$lambda.min

```

Como se puede observar en la gráfica el mejor $\lambda_l\ $ para la regresión Lasso es `r valnetlasso$lambda.min`

## 4- regresión Ridge y Lasso con valores $\lambda\ $ encontrados

```{r}
model_ridge <- glmnet(X,Y,alpha = 0, lambda = lambda_ridge)
model_lasso <- glmnet(X,Y, lambda = lambda_lasso)
```

## 5- Selección de modelos

```{r}
X_test <- as.matrix(test[,2:5001])
Y_test <- test[,1]

# Predecir valores con ambos modelos
pred_ridge <- predict(model_ridge, newx = X_test)
pred_lasso <- predict(model_lasso, newx = X_test)

# Calcular el Error Cuadrático Medio (ECM) para cada modelo
ecm_ridge <- mean((pred_ridge - Y_test)^2)
ecm_lasso <- mean((pred_lasso - Y_test)^2)

# Imprimir los resultados
cat("ECM para Ridge:", ecm_ridge, "\n")
cat("ECM para Lasso:", ecm_lasso, "\n")
```
De acuerdo a los resultas obtenidos, el modelo con mejor ECM es la regresión Lasso.


## 6- Ajustar modelo seleccionado a los 1200 datos

Se ajusta el modelo con el lambda encontrado con mejor ECM
```{r}
X <- as.matrix(datos[,2:5001])
y <- datos[,1]
model_lasso <- glmnet(X,y, lambda = lambda_lasso)
```


## 7- Grafica de las trazas de los coeficientes en función de la penalización para el modelo elegido en el punto anterior

Esta línea de código genera un gráfico que muestra las trazas de los coeficientes en función de λ. Por defecto, plot para objetos glmnet muestra los coeficientes en el eje y y los logaritmos de λ en el eje x. También se incluyen líneas de trazas de coeficientes para cada variable.
```{r}
lambda = seq(0.001,1,length=100)
fit_lasso <- glmnet(X,y, alpha = 0, lambda = lambda)

plot(fit_lasso)
```
Este gráfico es útil para visualizar cómo los coeficientes de las variables cambian a medida que se ajusta la penalización λ en el modelo Lasso. Puedes observar qué coeficientes se vuelven exactamente cero a medida que λ aumenta, lo que es característico del enfoque Lasso para la selección de variables.


## 8- Resultados obtenidos:
El rendimiento óptimo, evaluado mediante el Error Cuadrático Medio (ECM), se logra mediante la implementación de la regresión Lasso en comparación con la regresión Ridge. Esto sugiere la presencia de variables que no contribuyen de manera significativa al modelo; más bien, obstaculizan la capacidad de la regresión para ajustar los coeficientes de manera coherente según los datos recopilados. Este fenómeno se atribuye a la alta multicolinealidad presente en el conjunto de variables. Por otro lado, la regresión Lasso aborda eficazmente esta situación penalizando las variables menos relevantes, llevándolas a cero. Este enfoque transforma un modelo de alta dimensionalidad en uno más interpretable. Este fenómeno se refleja claramente en las trazas, donde numerosas variables han sido penalizadas hasta alcanzar el valor cero, lo que contribuye a una interpretación más precisa y eficaz del modelo. 

