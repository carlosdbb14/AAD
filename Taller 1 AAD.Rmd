---
title: "Taller 1 Analisis Avanzado de Datos"
subtitle: "Maestria en Matemáticas Aplicadas y Ciencias de la Computación"
author: 
  - "Carlos Daniel Barriga"
  - "Fabian Ricardo Luengas"
date: "2024-03-03"
output:
  rmdformats::downcute
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Taller 1

*Problema:*
El conjunto de datos *taller1*.txt contiene la información del perfíl genómico de un conjunto de 1200 líneas celulares. Para estas se busca determinar cuáles de los 5000 genes (ubicados en cada columna) son de relevancia para la predicción de la variable respuesta (efectividad del tratamiento anticancer, medida como variable continua).

```{r}
datos <- read.csv('taller1.txt')
```

## 1- Hay multicolinealidad en los datos?

si, hay multicolinealidad en los datos, debido a que hay mas variables que registros.

## 2- Separacion de los datos

```{r}
set.seed(47)

inds <- sample(1200,1000)

train <- datos[inds,]
test <- datos[-inds,]

```

## 3- validacion cruzada para $\lambda\ $ en Ridge y Lasso

Se usa Validación cruzada en k - folds para 5 folds

```{r warning=FALSE, echo=FALSE}
library(glmnet)
rownames(train) <- NULL
X <- as.matrix(train[,2:5001])
Y <- train[,1]

lambda = seq(0.001,1,length=100)
```


### 3.1 Regresión Ridge

```{r}

valnet <- cv.glmnet(X,Y, alpha = 0, nfolds = 5, lambda = lambda)

lambda_ridge = valnet$lambda.min
```

```{r}

plot(valnet$lambda,valnet$cvm, type = 'l', xlab = "Lambda", ylab = "ECM")
abline(v = valnet$lambda.min, col = 'red')

```

Como se puede observar en la gráfica el mejor $\lambda_r\ $ para la regresión ridge es `r valnet$lambda.min`

### 3.2 regresión Lasso

```{r warning=FALSE}

valnetlasso <- cv.glmnet(X,Y, nfolds = 5, lambda = lambda)

ecmls <- valnetlasso$cvm
```

```{r}

plot(valnetlasso$lambda,valnetlasso$cvm, type = 'l',xlab = "Lambda", ylab = "ECM")
abline(v = valnetlasso$lambda.min, col = 'red')

lambda_lasso = valnetlasso$lambda.min

```

Como se puede observar en la gráfica el mejor $\lambda_l\ $ para la regresión Lasso es `r valnetlasso$lambda.min`

## 4- regresión Ridge y Lasso con valores $\lambda\ $ encontrados

```{r}
model_ridge <- glmnet(X,Y,alpha = 0, lambda = lambda_ridge)
model_lasso <- glmnet(X,Y, lambda = lambda_lasso)
```

## 5- Selección de modelos

```{r}
X_test <- as.matrix(test[,2:5001])
Y_test <- test[,1]

# Predecir valores con ambos modelos
pred_ridge <- predict(model_ridge, newx = X_test)
pred_lasso <- predict(model_lasso, newx = X_test)

# Calcular el Error Cuadrático Medio (ECM) para cada modelo
ecm_ridge <- mean((pred_ridge - Y_test)^2)
ecm_lasso <- mean((pred_lasso - Y_test)^2)

# Imprimir los resultados
cat("ECM para Ridge:", ecm_ridge, "\n")
cat("ECM para Lasso:", ecm_lasso, "\n")
```
De acuerdo a los resultas obtenidos, el modelo con mejor ECM es la regresión Lasso.


## 6- Ajustar modelo seleccionado a los 1200 datos


```{r}
X <- as.matrix(datos[,2:5001])
y <- datos[,1]
model_lasso <- glmnet(X,y, lambda = lambda_lasso)
```

```{r}

```

