---
title: "Taller 2 Analisis Avanzado de Datos"
subtitle: "Maestria en Matemáticas Aplicadas y Ciencias de la Computación"
author: 
  - "Carlos Daniel Barriga"
  - "Fabian Ricardo Luengas"
date: "2024-03-03"
output:
  rmdformats::downcute
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Taller 2

**Problema 1**

El conjunto de datos Auto en la librería ISLR2, utilizado en clase, contiene la información del rendimiento y otras variables para un total de 392 vehículos. Como nos dimos cuenta, la relación entre dos de sus variables (horsepower y mpg) es resumida de manera parsimoniosa mediante un polinomio global de grado 2, sin embargo un spline suavizado (smoothing spline) parece dar un menor error de predicción. Por otra parte, determinar la ubicación y cantidad de knots en el spline de regresión (regression spline) fue un problema que desincentivó su uso. El método de validación externa utilizado para comprar los modelos fue validación regular.

```{r}
library('ISLR2')
library('splines')
library('boot')
```


## 1. Separe aleatoriamente su conjunto de datos en dos partes

```{r}

sample_size <- nrow(Auto)
set.seed(410)
train <- sample(sample_size, 0.9*sample_size)
test <- seq(sample_size)[!seq(sample_size) %in% train]
```


## 2. Determinar el numero de Knots para una regresión spline usando validación cruzada

```{r}
set.seed(410)
cv_error <- NULL
```


```{r}
train_data <- na.omit(Auto[train,c('mpg','horsepower')])
train_data$mpg <- as.numeric(train_data$mpg)
train_data$horsepower <- as.numeric(train_data$horsepower)
for (i in 1:10){
  glm_model <- glm(mpg ~ bs(horsepower, knots=i, Boundary.knots = range(horsepower) + c(-10,+10)), data = train_data)
  
  cv_res <- cv.glm(train_data,glm_model,K=10)
  cv_error[i] <- cv_res$delta[1]
}

mejor_knots <- which (cv_error == min(cv_error))

```
la validación cruzada en K folds evidencia que el menor ECM se da con `r mejor_knots` knots y un error de `r min(cv_error)`


## 3. Compara modelos para encontrar el mejor modelo en base de Funciones

### polinomio grado 2 global
```{r}
set.seed(410)
errores_base_funciones <- NULL

glm_model <- glm(mpg ~ poly(horsepower,2), data = train_data)
  
cv_res <- cv.glm(train_data,glm_model,K=10)
errores_base_funciones[1] <- cv_res$delta[1]
```

### polinomio b-spline ajustado 

```{r}
set.seed(410)
glm_model <- glm(mpg ~ bs(horsepower, knots=mejor_knots, Boundary.knots = range(horsepower) + c(-10,+10)), data = train_data)

cv_res <- cv.glm(train_data,glm_model,K=10)
errores_base_funciones[2] <- cv_res$delta[1]

```

### spline suavizado

```{r, warning=FALSE}
set.seed(410)
mse_sample <- NULL
sample_size_2 <- nrow(train_data) 
rnd_sample = sample(rep(1:10,length.out=sample_size_2))

for(i in 1:10){
  tr <- na.omit(train_data[rnd_sample != i,])
  tsr <- na.omit(train_data[rnd_sample == i,])
  mod_ss = smooth.spline(tr$horsepower, tr$mpg, cv = TRUE)
  mse_sample[i] <- mean((tsr$mpg - predict(mod_ss,tsr$horsepower)$y)**2)
}

errores_base_funciones[3] <- mean(mse_sample)
errores_base_funciones
```

segun validación cruzada el menor ECM se da con el modelo de spline suavizado que muestra un error de `r errores_base_funciones[3]`

## 4. Mejor Modelo de Regresión local

```{r}
set.seed(420)


local_model_2 <- loess(mpg ~ horsepower, degree = 2, data = train_data)
sample_size_2 <- nrow(train_data)
mse_1 <- NULL
rnd_sample = sample(rep(1:10,length.out=sample_size))
for(i in 1:10){
  tr <- na.omit(train_data[rnd_sample != i,])
  tsr <- na.omit(train_data[rnd_sample == i,])
  local_model_1 <- loess(mpg ~ horsepower, degree = 1, data = tr)
  mse_1[i] <- mean((tsr$mpg - predict(local_model_1,tsr$horsepower))**2, na.rm = TRUE)
  
}


sample_size_2 <- nrow(train_data)
mse_2 <- NULL
rnd_sample = sample(rep(1:10,length.out=sample_size_2))
for(i in 1:10){
  tr <- na.omit(train_data[rnd_sample != i,])
  tsr <- na.omit(train_data[rnd_sample == i,])
  local_model_2 <- loess(mpg ~ horsepower, degree = 2, data = tr)
  mse_2[i] <- mean((tsr$mpg - predict(local_model_2,tsr$horsepower))**2, na.rm = TRUE)
}

errores_locales <- NULL

errores_locales[1] <- mean(mse_1)
errores_locales[2] <- mean(mse_2)


```
el modelo con menor ECM es aquel hecho con una regresión de grado 1, que nos da un ECM de `r errores_locales[1]`

## 5. seleccionar el mejor de los 3 modelos

### con spline suavizado

```{r, warning = FALSE}
test_data <- Auto[test,c('mpg','horsepower')]

mod_spline = smooth.spline(train_data$horsepower, train_data$mpg, cv = TRUE)
spl_error <- mean((test_data$mpg - predict(mod_spline,test_data$horsepower)$y)**2)
spl_error
```

### Con Polinomios Locales

```{r}

local_model <- loess(mpg ~ horsepower, degree = 1, data = train_data)
local_error <- mean((test_data$mpg - predict(local_model,test_data$horsepower))**2)
local_error
```

### Polinomio regresión grado 2

```{r, warning=FALSE}
reg_model <- lm(mpg ~ poly(horsepower, 2), data = train_data)
reg_error <- mean((test_data$mpg - predict(reg_model,data.frame(horsepower = test_data$horsepower)))**2)
reg_error
```

corroborando con los datos de entrenamiento y prueba, tenemos como mejor modelo el spline suavizado que nos da un ECM  de `r spl_error`

## 6. Repita (1) - (5) un total de 10 veces de manera que en el paso (1) conforme una nueva muestra de validación cruzada, esto le permitiría obtener 10 ECM de prueba para cada paradigma de modelamiento. Grafique las tres distribuciones del ECM de prueba y responda ¿Cúal acercmiento seleccionaría basado en el ECM de predición: basado en base de funciones, basado en regresión local o polinomial global?.


```{r,echo=FALSE, warning=FALSE, message=FALSE}
sample_size <- nrow(Auto)

num_iteraciones <- 10

funciones_ECM <- numeric(0)

funciones_locales_ECM <- numeric(0)

funciones_pol2_ECM <- numeric(0)

for (i in 1:num_iteraciones)
{
  semilla <- set.seed(410+i)
  
  #cat("ITERACIÓN: ")
  #print(i)
  #cat("\n")
  
  train <- sample(sample_size, 0.9*sample_size)
  test <- seq(sample_size)[!seq(sample_size) %in% train]
  
  cv_error <- NULL
  
  train_data <- na.omit(Auto[train,c('mpg','horsepower')])
  train_data$mpg <- as.numeric(train_data$mpg)
  train_data$horsepower <- as.numeric(train_data$horsepower)
  for (i in 1:10){
    glm_model <- glm(mpg ~ bs(horsepower, knots=i, Boundary.knots = range(horsepower) + c(-10,+10)), data = train_data)
    
    cv_res <- cv.glm(train_data,glm_model,K=10)
    cv_error[i] <- cv_res$delta[1]
  }
  
  mejor_knots <- which (cv_error == min(cv_error))
  
  set.seed(410)
  errores_base_funciones <- NULL
  glm_model <- glm(mpg ~ poly(horsepower,2), data = train_data)
  
  ### polinomio grado 2 global
  cv_res <- cv.glm(train_data,glm_model,K=10)
  errores_base_funciones[1] <- cv_res$delta[1]
  
  set.seed(410)
  glm_model <- glm(mpg ~ bs(horsepower, knots=mejor_knots, Boundary.knots = range(horsepower) + c(-10,+10)), data = train_data)
  
  ### polinomio b-spline ajustado 
  cv_res <- cv.glm(train_data,glm_model,K=10)
  errores_base_funciones[2] <- cv_res$delta[1]
  
  set.seed(410)
  mse_sample <- NULL
  sample_size_2 <- nrow(train_data) 
  rnd_sample = sample(rep(1:10,length.out=sample_size_2))
  
  for(i in 1:10){
    tr <- na.omit(train_data[rnd_sample != i,])
    tsr <- na.omit(train_data[rnd_sample == i,])
    mod_ss = smooth.spline(tr$horsepower, tr$mpg, cv = TRUE)
    mse_sample[i] <- mean((tsr$mpg - predict(mod_ss,tsr$horsepower)$y)**2)
  }
  
  ### Modelo basado en funciones
  errores_base_funciones[3] <- mean(mse_sample)
  
  #cat("Modelo basado en funciones:\n")
  #print(min(errores_base_funciones))
  #cat("\n")
  
  funciones_ECM <- append(funciones_ECM,mean(errores_base_funciones))
  
  ### Mejor Modelo de Regresión local
  
  local_model_2 <- loess(mpg ~ horsepower, degree = 2, data = train_data)
  sample_size_2 <- nrow(train_data)
  mse_1 <- NULL
  rnd_sample = sample(rep(1:10,length.out=sample_size))
  for(i in 1:10){
    tr <- na.omit(train_data[rnd_sample != i,])
    tsr <- na.omit(train_data[rnd_sample == i,])
    local_model_1 <- loess(mpg ~ horsepower, degree = 1, data = tr)
    mse_1[i] <- mean((tsr$mpg - predict(local_model_1,tsr$horsepower))**2, na.rm = TRUE)
  }
  
  sample_size_2 <- nrow(train_data)
  mse_2 <- NULL
  rnd_sample = sample(rep(1:10,length.out=sample_size_2))
  for(i in 1:10){
    tr <- na.omit(train_data[rnd_sample != i,])
    tsr <- na.omit(train_data[rnd_sample == i,])
    local_model_2 <- loess(mpg ~ horsepower, degree = 2, data = tr)
    mse_2[i] <- mean((tsr$mpg - predict(local_model_2,tsr$horsepower))**2, na.rm = TRUE)
  }
  
  ## Mejor Modelo de Regresión local
  errores_locales <- NULL
  errores_locales[1] <- mean(mse_1)
  errores_locales[2] <- mean(mse_2)
  
  #cat("Mejor Modelo de Regresión local:\n")
  #print(min(errores_locales))
  #cat("\n")
  
  funciones_locales_ECM <- append(funciones_locales_ECM,mean(errores_locales))
  
  
  reg_model <- lm(mpg ~ poly(horsepower, 2), data = train_data)
  reg_error <- mean((test_data$mpg - predict(reg_model,data.frame(horsepower = test_data$horsepower)))**2)
  
  funciones_pol2_ECM <- append(funciones_pol2_ECM,reg_error)
  
}
cat("Resultado basado en Funciones:\n")
print(mean(funciones_ECM))
cat("Regresión local:\n")
print(mean(funciones_locales_ECM))
cat("Polinomica grado 2:\n")
print(mean(funciones_pol2_ECM))

```
De acuerdo a los resultados se selecciona el modelo de regresión Local

# **Problema 2**

En el contexto de análisis de datos funcionales se tiene una colección finita de observaciones ruidosas, donde para cada individuo, estas se asumen provenientes de una curva de dimensión infinita la cual es evaluada en puntos de un intervalo determinado. Para la i-ésima unidad estadística se tiene un conjunto de $n_i$ observaciones discretizadas $x_{i1} , ..., x_{ij} , ..., x_{in_i}$ de la función $x_i$ en los puntos $t_{i1}, ..., t_{ij} , ..., t_{in_i}$ con $x_{ij} \in \mathbb{R}$, $t_{ij} \in T$ y T un intervalo que representa el dominio sobre los reales donde se definen los datos funcionales.

Problema 2 - 20 pts. En el contexto de an´alisis de datos funcionales se tiene una colección finita de observaciones ruidosas, donde para cada individuo, estas se asumen provenientes de una curva de dimensión infinita la cual es evaluada en puntos de un intervalo determinado. Para la i-ésima unidad estadística se tiene un conjunto de ni observaciones discretizadas xi1, ..., xij , ..., xini de la función xi en los puntos ti1, ..., tij , ..., tini con xij ∈ R, tij ∈ T y T un intervalo que representa el dominio sobre los reales donde se definen los datos funcionales.

## 7. Escriba el estimador de Nadarya–Watson para la i-ésima unidad estadística en t, es decir, x(t).

La centralidad de los datos funcionales se resume en la funci´on media μ, la cual puede interpretarse en cada valor t ∈ T como el valor promedio de la funci´on aleatoria subyacente en t, μ(t). F´ıjese que el estimador de Nadarya–Watson puede extenderse a m´as de una unidad estad´ıstica, resultando en t como un promedio ponderado de las observaciones cercanas para todas las observaciones xij :

```{r}
# Generar datos de ejemplo
set.seed(123)  # Para reproducibilidad
n <- 100  # Número de observaciones
ti <- seq(0, 10, length.out = n)  # Puntos de la función
xi <- sin(ti) + rnorm(n, mean = 0, sd = 0.2)  # Datos generados con ruido

# Calcular el estimador de Nadaraya-Watson utilizando loess()
nw_model_1 <- loess(xi ~ ti, span = 0.5, degree = 1)  # Ajustar el modelo loess

nw_model_2 <- loess(xi ~ ti, span = 0.5, degree = 2)  # Ajustar el modelo loess
```
Se ajusta el ancho de banda con 0.5 el cual es utilizado para ajustar la curva de regresión localmente ponderada, con esto se busca un punto medio en el suavizado de la función 

## 8. Escriba el estimador de Nadarya–Watson para la función media en t, es decir, ˆμ(t). Note que todos los datos discretizados son utilizados en la estimaci´on de la funci´on media.


```{r}
# Punto en el que se desea estimar la función
t <- 1
estimacion_media1  <- predict(nw_model_1, newdata = data.frame(ti = t))  # Predecir en el punto t
estimacion_media2  <- predict(nw_model_2, newdata = data.frame(ti = t))  # Predecir en el punto t
# Imprimir la estimación
print(paste("Estimación de la función1 media en el punto t:", estimacion_media1))


# Imprimir la estimación
print(paste("Estimación de la función2 media en el punto t:", estimacion_media2))
```


